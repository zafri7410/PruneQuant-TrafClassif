#####################################################################################
# Floating point model  #############################################################
#####################################################################################
CNN1D_TrafficClassification(
  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=same)
  (bn1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=same)
  (bn2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (conv3): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=same)
  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=same)
  (bn4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv5): Conv1d(32, 16, kernel_size=(1,), stride=(1,), padding=same)
  (relu): ReLU()
)

#####################################################################################
# Prepared model ####################################################################
#####################################################################################
GraphModule(
  (activation_post_process_0): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (conv1): ConvBnReLU1d(
    1, 16, kernel_size=(3,), stride=(1,), padding=(1,)
    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (activation_post_process_1): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (pool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (activation_post_process_2): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (conv2): ConvBnReLU1d(
    16, 16, kernel_size=(3,), stride=(1,), padding=(1,)
    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (activation_post_process_3): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (activation_post_process_4): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (conv3): ConvBnReLU1d(
    16, 32, kernel_size=(3,), stride=(1,), padding=(1,)
    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (activation_post_process_5): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (activation_post_process_6): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (conv4): ConvBnReLU1d(
    32, 32, kernel_size=(3,), stride=(1,), padding=(1,)
    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (activation_post_process_7): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (activation_post_process_8): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (conv5): Conv1d(
    32, 16, kernel_size=(1,), stride=(1,)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
    )
  )
  (activation_post_process_9): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (activation_post_process_10): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
  (activation_post_process_11): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)
  )
)



def forward(self, x):
    activation_post_process_0 = self.activation_post_process_0(x);  x = None
    conv1 = self.conv1(activation_post_process_0);  activation_post_process_0 = None
    activation_post_process_1 = self.activation_post_process_1(conv1);  conv1 = None
    pool1 = self.pool1(activation_post_process_1);  activation_post_process_1 = None
    activation_post_process_2 = self.activation_post_process_2(pool1);  pool1 = None
    conv2 = self.conv2(activation_post_process_2);  activation_post_process_2 = None
    activation_post_process_3 = self.activation_post_process_3(conv2);  conv2 = None
    pool2 = self.pool2(activation_post_process_3);  activation_post_process_3 = None
    activation_post_process_4 = self.activation_post_process_4(pool2);  pool2 = None
    conv3 = self.conv3(activation_post_process_4);  activation_post_process_4 = None
    activation_post_process_5 = self.activation_post_process_5(conv3);  conv3 = None
    pool3 = self.pool3(activation_post_process_5);  activation_post_process_5 = None
    activation_post_process_6 = self.activation_post_process_6(pool3);  pool3 = None
    conv4 = self.conv4(activation_post_process_6);  activation_post_process_6 = None
    activation_post_process_7 = self.activation_post_process_7(conv4);  conv4 = None
    pool4 = self.pool4(activation_post_process_7);  activation_post_process_7 = None
    activation_post_process_8 = self.activation_post_process_8(pool4);  pool4 = None
    conv5 = self.conv5(activation_post_process_8);  activation_post_process_8 = None
    activation_post_process_9 = self.activation_post_process_9(conv5);  conv5 = None
    mean = torch.mean(activation_post_process_9, dim = -1, keepdim = True);  activation_post_process_9 = None
    activation_post_process_10 = self.activation_post_process_10(mean);  mean = None
    getattr_1 = activation_post_process_10.shape
    getitem = getattr_1[0];  getattr_1 = None
    reshape = activation_post_process_10.reshape(getitem, -1);  activation_post_process_10 = getitem = None
    activation_post_process_11 = self.activation_post_process_11(reshape);  reshape = None
    return activation_post_process_11



#####################################################################################
# Calibrated model ##################################################################
#####################################################################################
GraphModule(
  (activation_post_process_0): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0079], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=1.0)
  )
  (conv1): ConvBnReLU1d(
    1, 16, kernel_size=(3,), stride=(1,), padding=(1,)
    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0554], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=-7.060542583465576, max_val=2.7261221408843994)
    )
  )
  (activation_post_process_1): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0247], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=3.1320767402648926)
  )
  (pool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (activation_post_process_2): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0247], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=3.1320767402648926)
  )
  (conv2): ConvBnReLU1d(
    16, 16, kernel_size=(3,), stride=(1,), padding=(1,)
    (bn): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0157], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=-1.7015702724456787, max_val=2.0036911964416504)
    )
  )
  (activation_post_process_3): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0774], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=9.826797485351562)
  )
  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (activation_post_process_4): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0774], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=9.826797485351562)
  )
  (conv3): ConvBnReLU1d(
    16, 32, kernel_size=(3,), stride=(1,), padding=(1,)
    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0234], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=-2.979923725128174, max_val=2.3364992141723633)
    )
  )
  (activation_post_process_5): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1160], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=14.730525970458984)
  )
  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (activation_post_process_6): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1160], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=14.730525970458984)
  )
  (conv4): ConvBnReLU1d(
    32, 32, kernel_size=(3,), stride=(1,), padding=(1,)
    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1339], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=-13.34618091583252, max_val=17.069313049316406)
    )
  )
  (activation_post_process_7): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6883], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=87.41814422607422)
  )
  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (activation_post_process_8): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.6883], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=87.41814422607422)
  )
  (conv5): Conv1d(
    32, 16, kernel_size=(1,), stride=(1,)
    (weight_fake_quant): FakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0298], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
      (activation_post_process): MinMaxObserver(min_val=-3.795598030090332, max_val=3.3877570629119873)
    )
  )
  (activation_post_process_9): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.8617], device='cuda:0'), zero_point=tensor([76], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=-670.1385498046875, max_val=455.29254150390625)
  )
  (activation_post_process_10): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.8617], device='cuda:0'), zero_point=tensor([76], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=-670.1385498046875, max_val=455.29254150390625)
  )
  (activation_post_process_11): FakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([8.8617], device='cuda:0'), zero_point=tensor([76], device='cuda:0', dtype=torch.int32)
    (activation_post_process): MinMaxObserver(min_val=-670.1385498046875, max_val=455.29254150390625)
  )
)



def forward(self, x):
    activation_post_process_0 = self.activation_post_process_0(x);  x = None
    conv1 = self.conv1(activation_post_process_0);  activation_post_process_0 = None
    activation_post_process_1 = self.activation_post_process_1(conv1);  conv1 = None
    pool1 = self.pool1(activation_post_process_1);  activation_post_process_1 = None
    activation_post_process_2 = self.activation_post_process_2(pool1);  pool1 = None
    conv2 = self.conv2(activation_post_process_2);  activation_post_process_2 = None
    activation_post_process_3 = self.activation_post_process_3(conv2);  conv2 = None
    pool2 = self.pool2(activation_post_process_3);  activation_post_process_3 = None
    activation_post_process_4 = self.activation_post_process_4(pool2);  pool2 = None
    conv3 = self.conv3(activation_post_process_4);  activation_post_process_4 = None
    activation_post_process_5 = self.activation_post_process_5(conv3);  conv3 = None
    pool3 = self.pool3(activation_post_process_5);  activation_post_process_5 = None
    activation_post_process_6 = self.activation_post_process_6(pool3);  pool3 = None
    conv4 = self.conv4(activation_post_process_6);  activation_post_process_6 = None
    activation_post_process_7 = self.activation_post_process_7(conv4);  conv4 = None
    pool4 = self.pool4(activation_post_process_7);  activation_post_process_7 = None
    activation_post_process_8 = self.activation_post_process_8(pool4);  pool4 = None
    conv5 = self.conv5(activation_post_process_8);  activation_post_process_8 = None
    activation_post_process_9 = self.activation_post_process_9(conv5);  conv5 = None
    mean = torch.mean(activation_post_process_9, dim = -1, keepdim = True);  activation_post_process_9 = None
    activation_post_process_10 = self.activation_post_process_10(mean);  mean = None
    getattr_1 = activation_post_process_10.shape
    getitem = getattr_1[0];  getattr_1 = None
    reshape = activation_post_process_10.reshape(getitem, -1);  activation_post_process_10 = getitem = None
    activation_post_process_11 = self.activation_post_process_11(reshape);  reshape = None
    return activation_post_process_11

#####################################################################################
# Converted model using quantize_fx #################################################
#####################################################################################
GraphModule(
  (conv1): QuantizedConvReLU1d(1, 16, kernel_size=(3,), stride=(1,), scale=0.01118683535605669, zero_point=0, padding=(1,))
  (pool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (conv2): QuantizedConvReLU1d(16, 16, kernel_size=(3,), stride=(1,), scale=0.015973301604390144, zero_point=0, padding=(1,))
  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (conv3): QuantizedConvReLU1d(16, 32, kernel_size=(3,), stride=(1,), scale=0.026778021827340126, zero_point=0, padding=(1,))
  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv4): QuantizedConvReLU1d(32, 32, kernel_size=(3,), stride=(1,), scale=0.5226547122001648, zero_point=0, padding=(1,))
  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv5): QuantizedConv1d(32, 16, kernel_size=(1,), stride=(1,), scale=7.148700714111328, zero_point=171)
)



def forward(self, x):
    conv1_input_scale_0 = self.conv1_input_scale_0
    conv1_input_zero_point_0 = self.conv1_input_zero_point_0
    quantize_per_tensor = torch.quantize_per_tensor(x, conv1_input_scale_0, conv1_input_zero_point_0, torch.quint8);  x = conv1_input_scale_0 = conv1_input_zero_point_0 = None
    conv1 = self.conv1(quantize_per_tensor);  quantize_per_tensor = None
    pool1 = self.pool1(conv1);  conv1 = None
    conv2 = self.conv2(pool1);  pool1 = None
    pool2 = self.pool2(conv2);  conv2 = None
    conv3 = self.conv3(pool2);  pool2 = None
    pool3 = self.pool3(conv3);  conv3 = None
    conv4 = self.conv4(pool3);  pool3 = None
    pool4 = self.pool4(conv4);  conv4 = None
    conv5 = self.conv5(pool4);  pool4 = None
    mean = torch.mean(conv5, dim = -1, keepdim = True);  conv5 = None
    getattr_1 = mean.shape
    getitem = getattr_1[0];  getattr_1 = None
    reshape = mean.reshape(getitem, -1);  mean = getitem = None
    dequantize_11 = reshape.dequantize();  reshape = None
    return dequantize_11


#####################################################################################
# ONNX torch.onnx.export the converted fx ###########################################
#####################################################################################
# Most likely not working as intended ###############################################
#####################################################################################
graph main_graph (
  %input[FLOAT, 1024x1x1500]
) initializers (
  %onnx::QuantizeLinear_194[UINT8, scalar]
  %onnx::QuantizeLinear_195[FLOAT, scalar]
  %onnx::ConstantOfShape_202[INT64, 1]
  %onnx::DequantizeLinear_205[INT32, 16]
  %onnx::DequantizeLinear_206[FLOAT, 1]
) {
  %onnx::DequantizeLinear_197 = Identity(%onnx::QuantizeLinear_194)
  %onnx::DequantizeLinear_196 = Identity(%onnx::QuantizeLinear_195)
  %/QuantizeLinear_output_0 = QuantizeLinear(%input, %onnx::QuantizeLinear_195, %onnx::QuantizeLinear_194)
  %/conv1/Cast_output_0 = Cast[to = 2](%/QuantizeLinear_output_0)
  %/conv1/DequantizeLinear_output_0 = DequantizeLinear(%/conv1/Cast_output_0, %onnx::DequantizeLinear_196, %onnx::DequantizeLinear_197)
  %/conv1/Constant_output_0 = Constant[value = <Tensor>]()
  %/conv1/Constant_1_output_0 = Constant[value = <Tensor>]()
  %/conv1/Constant_2_output_0 = Constant[value = <Tensor>]()
  %/conv1/DequantizeLinear_1_output_0 = DequantizeLinear(%/conv1/Constant_output_0, %/conv1/Constant_1_output_0, %/conv1/Constant_2_output_0)
  %/conv1/ConstantOfShape_output_0 = ConstantOfShape[value = <Tensor>](%onnx::ConstantOfShape_202)
  %/conv1/Cast_1_output_0 = Cast[to = 6](%/conv1/ConstantOfShape_output_0)
  %/conv1/DequantizeLinear_2_output_0 = DequantizeLinear(%onnx::DequantizeLinear_205, %onnx::DequantizeLinear_206, %/conv1/Cast_1_output_0)
  %/conv1/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/conv1/DequantizeLinear_output_0, %/conv1/DequantizeLinear_1_output_0, %/conv1/DequantizeLinear_2_output_0)
  %/conv1/Relu_output_0 = Relu(%/conv1/Conv_output_0)
  %/conv1/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv1/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv1/QuantizeLinear_output_0 = QuantizeLinear(%/conv1/Relu_output_0, %/conv1/Constant_3_output_0, %/conv1/Constant_4_output_0)
  %/pool1/Cast_output_0 = Cast[to = 2](%/conv1/QuantizeLinear_output_0)
  %/pool1/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/pool1/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/pool1/DequantizeLinear_output_0 = DequantizeLinear(%/pool1/Cast_output_0, %/pool1/Constant_output_0, %/pool1/Constant_1_output_0)
  %/pool1/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1], kernel_shape = [3], pads = [0, 0], strides = [3]](%/pool1/DequantizeLinear_output_0)
  %/pool1/QuantizeLinear_output_0 = QuantizeLinear(%/pool1/MaxPool_output_0, %/pool1/Constant_output_0, %/pool1/Constant_1_output_0)
  %/conv2/Cast_output_0 = Cast[to = 2](%/pool1/QuantizeLinear_output_0)
  %/conv2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv2/DequantizeLinear_output_0 = DequantizeLinear(%/conv2/Cast_output_0, %/conv2/Constant_output_0, %/conv2/Constant_1_output_0)
  %/conv2/Constant_2_output_0 = Constant[value = <Tensor>]()
  %/conv2/Constant_3_output_0 = Constant[value = <Tensor>]()
  %/conv2/Constant_4_output_0 = Constant[value = <Tensor>]()
  %/conv2/DequantizeLinear_1_output_0 = DequantizeLinear(%/conv2/Constant_2_output_0, %/conv2/Constant_3_output_0, %/conv2/Constant_4_output_0)
  %/conv2/Constant_5_output_0 = Constant[value = <Tensor>]()
  %/conv2/ConstantOfShape_output_0 = ConstantOfShape[value = <Tensor>](%/conv2/Constant_5_output_0)
  %/conv2/Constant_6_output_0 = Constant[value = <Tensor>]()
  %/conv2/Constant_7_output_0 = Constant[value = <Tensor>]()
  %/conv2/Cast_1_output_0 = Cast[to = 6](%/conv2/ConstantOfShape_output_0)
  %/conv2/DequantizeLinear_2_output_0 = DequantizeLinear(%/conv2/Constant_6_output_0, %/conv2/Constant_7_output_0, %/conv2/Cast_1_output_0)
  %/conv2/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/conv2/DequantizeLinear_output_0, %/conv2/DequantizeLinear_1_output_0, %/conv2/DequantizeLinear_2_output_0)
  %/conv2/Relu_output_0 = Relu(%/conv2/Conv_output_0)
  %/conv2/Constant_8_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv2/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv2/QuantizeLinear_output_0 = QuantizeLinear(%/conv2/Relu_output_0, %/conv2/Constant_8_output_0, %/conv2/Constant_9_output_0)
  %/pool2/Cast_output_0 = Cast[to = 2](%/conv2/QuantizeLinear_output_0)
  %/pool2/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/pool2/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/pool2/DequantizeLinear_output_0 = DequantizeLinear(%/pool2/Cast_output_0, %/pool2/Constant_output_0, %/pool2/Constant_1_output_0)
  %/pool2/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1], kernel_shape = [3], pads = [0, 0], strides = [3]](%/pool2/DequantizeLinear_output_0)
  %/pool2/QuantizeLinear_output_0 = QuantizeLinear(%/pool2/MaxPool_output_0, %/pool2/Constant_output_0, %/pool2/Constant_1_output_0)
  %/conv3/Cast_output_0 = Cast[to = 2](%/pool2/QuantizeLinear_output_0)
  %/conv3/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv3/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv3/DequantizeLinear_output_0 = DequantizeLinear(%/conv3/Cast_output_0, %/conv3/Constant_output_0, %/conv3/Constant_1_output_0)
  %/conv3/Constant_2_output_0 = Constant[value = <Tensor>]()
  %/conv3/Constant_3_output_0 = Constant[value = <Tensor>]()
  %/conv3/Constant_4_output_0 = Constant[value = <Tensor>]()
  %/conv3/DequantizeLinear_1_output_0 = DequantizeLinear(%/conv3/Constant_2_output_0, %/conv3/Constant_3_output_0, %/conv3/Constant_4_output_0)
  %/conv3/Constant_5_output_0 = Constant[value = <Tensor>]()
  %/conv3/ConstantOfShape_output_0 = ConstantOfShape[value = <Tensor>](%/conv3/Constant_5_output_0)
  %/conv3/Constant_6_output_0 = Constant[value = <Tensor>]()
  %/conv3/Constant_7_output_0 = Constant[value = <Tensor>]()
  %/conv3/Cast_1_output_0 = Cast[to = 6](%/conv3/ConstantOfShape_output_0)
  %/conv3/DequantizeLinear_2_output_0 = DequantizeLinear(%/conv3/Constant_6_output_0, %/conv3/Constant_7_output_0, %/conv3/Cast_1_output_0)
  %/conv3/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/conv3/DequantizeLinear_output_0, %/conv3/DequantizeLinear_1_output_0, %/conv3/DequantizeLinear_2_output_0)
  %/conv3/Relu_output_0 = Relu(%/conv3/Conv_output_0)
  %/conv3/Constant_8_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv3/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv3/QuantizeLinear_output_0 = QuantizeLinear(%/conv3/Relu_output_0, %/conv3/Constant_8_output_0, %/conv3/Constant_9_output_0)
  %/pool3/Cast_output_0 = Cast[to = 2](%/conv3/QuantizeLinear_output_0)
  %/pool3/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/pool3/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/pool3/DequantizeLinear_output_0 = DequantizeLinear(%/pool3/Cast_output_0, %/pool3/Constant_output_0, %/pool3/Constant_1_output_0)
  %/pool3/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1], kernel_shape = [2], pads = [0, 0], strides = [2]](%/pool3/DequantizeLinear_output_0)
  %/pool3/QuantizeLinear_output_0 = QuantizeLinear(%/pool3/MaxPool_output_0, %/pool3/Constant_output_0, %/pool3/Constant_1_output_0)
  %/conv4/Cast_output_0 = Cast[to = 2](%/pool3/QuantizeLinear_output_0)
  %/conv4/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv4/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv4/DequantizeLinear_output_0 = DequantizeLinear(%/conv4/Cast_output_0, %/conv4/Constant_output_0, %/conv4/Constant_1_output_0)
  %/conv4/Constant_2_output_0 = Constant[value = <Tensor>]()
  %/conv4/Constant_3_output_0 = Constant[value = <Tensor>]()
  %/conv4/Constant_4_output_0 = Constant[value = <Tensor>]()
  %/conv4/DequantizeLinear_1_output_0 = DequantizeLinear(%/conv4/Constant_2_output_0, %/conv4/Constant_3_output_0, %/conv4/Constant_4_output_0)
  %/conv4/Constant_5_output_0 = Constant[value = <Tensor>]()
  %/conv4/ConstantOfShape_output_0 = ConstantOfShape[value = <Tensor>](%/conv4/Constant_5_output_0)
  %/conv4/Constant_6_output_0 = Constant[value = <Tensor>]()
  %/conv4/Constant_7_output_0 = Constant[value = <Tensor>]()
  %/conv4/Cast_1_output_0 = Cast[to = 6](%/conv4/ConstantOfShape_output_0)
  %/conv4/DequantizeLinear_2_output_0 = DequantizeLinear(%/conv4/Constant_6_output_0, %/conv4/Constant_7_output_0, %/conv4/Cast_1_output_0)
  %/conv4/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [3], pads = [1, 1], strides = [1]](%/conv4/DequantizeLinear_output_0, %/conv4/DequantizeLinear_1_output_0, %/conv4/DequantizeLinear_2_output_0)
  %/conv4/Relu_output_0 = Relu(%/conv4/Conv_output_0)
  %/conv4/Constant_8_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv4/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv4/QuantizeLinear_output_0 = QuantizeLinear(%/conv4/Relu_output_0, %/conv4/Constant_8_output_0, %/conv4/Constant_9_output_0)
  %/pool4/Cast_output_0 = Cast[to = 2](%/conv4/QuantizeLinear_output_0)
  %/pool4/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/pool4/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/pool4/DequantizeLinear_output_0 = DequantizeLinear(%/pool4/Cast_output_0, %/pool4/Constant_output_0, %/pool4/Constant_1_output_0)
  %/pool4/MaxPool_output_0 = MaxPool[ceil_mode = 0, dilations = [1], kernel_shape = [2], pads = [0, 0], strides = [2]](%/pool4/DequantizeLinear_output_0)
  %/pool4/QuantizeLinear_output_0 = QuantizeLinear(%/pool4/MaxPool_output_0, %/pool4/Constant_output_0, %/pool4/Constant_1_output_0)
  %/conv5/Cast_output_0 = Cast[to = 2](%/pool4/QuantizeLinear_output_0)
  %/conv5/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv5/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv5/DequantizeLinear_output_0 = DequantizeLinear(%/conv5/Cast_output_0, %/conv5/Constant_output_0, %/conv5/Constant_1_output_0)
  %/conv5/Constant_2_output_0 = Constant[value = <Tensor>]()
  %/conv5/Constant_3_output_0 = Constant[value = <Tensor>]()
  %/conv5/Constant_4_output_0 = Constant[value = <Tensor>]()
  %/conv5/DequantizeLinear_1_output_0 = DequantizeLinear(%/conv5/Constant_2_output_0, %/conv5/Constant_3_output_0, %/conv5/Constant_4_output_0)
  %/conv5/Constant_5_output_0 = Constant[value = <Tensor>]()
  %/conv5/ConstantOfShape_output_0 = ConstantOfShape[value = <Tensor>](%/conv5/Constant_5_output_0)
  %/conv5/Constant_6_output_0 = Constant[value = <Tensor>]()
  %/conv5/Constant_7_output_0 = Constant[value = <Tensor>]()
  %/conv5/Cast_1_output_0 = Cast[to = 6](%/conv5/ConstantOfShape_output_0)
  %/conv5/DequantizeLinear_2_output_0 = DequantizeLinear(%/conv5/Constant_6_output_0, %/conv5/Constant_7_output_0, %/conv5/Cast_1_output_0)
  %/conv5/Conv_output_0 = Conv[dilations = [1], group = 1, kernel_shape = [1], pads = [0, 0], strides = [1]](%/conv5/DequantizeLinear_output_0, %/conv5/DequantizeLinear_1_output_0, %/conv5/DequantizeLinear_2_output_0)
  %/conv5/Constant_8_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv5/Constant_9_output_0 = Constant[value = <Scalar Tensor []>]()
  %/conv5/QuantizeLinear_output_0 = QuantizeLinear(%/conv5/Conv_output_0, %/conv5/Constant_8_output_0, %/conv5/Constant_9_output_0)
  %/Cast_output_0 = Cast[to = 2](%/conv5/QuantizeLinear_output_0)
  %/Constant_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Constant_1_output_0 = Constant[value = <Scalar Tensor []>]()
  %/DequantizeLinear_output_0 = DequantizeLinear(%/Cast_output_0, %/Constant_output_0, %/Constant_1_output_0)
  %/ReduceMean_output_0 = ReduceMean[axes = [-1], keepdims = 1](%/DequantizeLinear_output_0)
  %/QuantizeLinear_1_output_0 = QuantizeLinear(%/ReduceMean_output_0, %/Constant_output_0, %/Constant_1_output_0)
  %/Constant_2_output_0 = Constant[value = <Tensor>]()
  %/Cast_1_output_0 = Cast[to = 2](%/QuantizeLinear_1_output_0)
  %/Constant_3_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Constant_4_output_0 = Constant[value = <Scalar Tensor []>]()
  %/DequantizeLinear_1_output_0 = DequantizeLinear(%/Cast_1_output_0, %/Constant_3_output_0, %/Constant_4_output_0)
  %/Reshape_output_0 = Reshape[allowzero = 0](%/DequantizeLinear_1_output_0, %/Constant_2_output_0)
  %/QuantizeLinear_2_output_0 = QuantizeLinear(%/Reshape_output_0, %/Constant_3_output_0, %/Constant_4_output_0)
  %/Cast_2_output_0 = Cast[to = 2](%/QuantizeLinear_2_output_0)
  %/Constant_5_output_0 = Constant[value = <Scalar Tensor []>]()
  %/Constant_6_output_0 = Constant[value = <Scalar Tensor []>]()
  %output = DequantizeLinear(%/Cast_2_output_0, %/Constant_5_output_0, %/Constant_6_output_0)
  return %output
}











    